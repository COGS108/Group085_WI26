{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with your team list and their contributions. Note that this will change over the course of the checkpoints\n",
    "\n",
    "This is a modified [CRediT taxonomy of contributions](https://credit.niso.org). For each group member please list how they contributed to this project using these terms:\n",
    "> Analysis, Background research, Conceptualization, Data curation, Experimental investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review & editing\n",
    "\n",
    "Example team list and credits:\n",
    "- Alice Anderson: Conceptualization, Data curation, Methodology, Writing - original draft\n",
    "- Bob Barker:  Analysis, Software, Visualization\n",
    "- Charlie Chang: Project administration, Software, Writing - review & editing\n",
    "- Dani Delgado: Analysis, Background research, Visualization, Writing - original draft\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Anusha Malla: Project administration, Software, Writing - review & editing\n",
    "- Fatimah Alhumrani:  Conceptualization, Analysis, Software, Visualization\n",
    "- Hanwen Chen: Project administration, Software, Background research, Writing - review & editing\n",
    "- Nawaf Ahmed: Conceptualization, Data curation, Methodology, Writing - original draft\n",
    "- Tom Klappenecker: Conceptualization, Analysis, Data curation, Visualization, Writing - original draft"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n",
    "\n",
    "Is food desert status associated with differential access to McDonald’s versus Chipotle restaurants across U.S. census tracts? Specifically, do food desert census tracts have a shorter average distance to McDonald’s locations and a lower likelihood of containing a Chipotle location compared to non-food desert tracts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n",
    "\n",
    "Food deserts refer to areas where residents' access to affordable, healthy food options is restricted or nonexistent due to the absence of grocery stores within convenient traveling distance. According to Food Empowerment Project's description, people’s choices about what to eat are severely limited by the options available to them and what they can afford, and many food deserts contain an overabundance of fast food chains selling cheap “meat” and dairy-based foods that are high in fat, sugar and salt.<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1)\n",
    "\n",
    "Previous studies have identified socio-economic factors that correlates with the presence of food desert. In *Characteristics and Influential Factors of Food Deserts*<a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2), the authors find that areas with higher level of poverty are more likely to become food deserts, regardless of urban or rural designation of that area. For other factors, such as vehicle availability and use of public transportations, the association with food desert status varies across very dense urban areas, less dense urban areas, and rural areas. This study has shown that in less urban and rural areas, the percentage of racial minority population affects the food desert status of an area, where food deserts tracts have a greater concentration of minorities. The study has also shown differences across geographical locations within the United States, where low-income populations in some regions are less vulnerable to access problems than low income populations in other regions. However, problems exist in this study. One such problem is that during the regression analysis, the study only examines the food deserts status of the census tracts that are classified as low income. While low-income and food access jointly define food desert status, analyzing the two properties concurrently can distort the relationship between poverty-related variables, such as unemployment rate and poverty rate, and food desert status.\n",
    "\n",
    "An article published by Addy Yater aimed to examine the difference in grocery accessibility between food deserts and non-food deserts in Indianapolis, Indiana<a name=\"cite_ref-3\"></a><sup>3</sup>. In the article, Yater found that poorer areas contained no grocery stores, while wealthier areas had plenty. A limitation of Yater's study, however, is that it focuses exclusively on regions within Indianapolis, Indiana, which may not be representative of the entire United States. Another problem is that, like the previous study, Yater associates low-income areas with food deserts, which can confound the measurement of other variables within the region. In our study, we aim to compare the accessibility of healthy versus unhealthy foods directly in each region, regardless of whether it is classified as a food desert. While McDonald’s is generally regarded as unhealthy, Chipotle is often considered a healthier alternative within the fast-food industry. Therefore, we chose McDonald’s and Chipotle as the unhealthy and healthy food options, respectively, to compare their accessibility in both food desert and non-food desert areas.\n",
    "\n",
    "1. <a name=\"cite_note-1\"></a> Food Empowerment Project. *Food Deserts*. FoodIsPower.org. 2026. https://foodispower.org/access-health/food-deserts/\n",
    "2. <a name=\"cite_note-2\"></a> Dutko, P., Ver Ploeg, M., & Farrigan, T. (2012) *Characteristics and Influential Factors of Food Deserts*, Economic Research Service, U.S. Department of Agriculture, ERR-140. https://www.ers.usda.gov/publications/pub-details?pubid=45017\n",
    "3.  <a name=\"cite_note-3\"></a>Yater, A. *Correlation Between Food Deserts and Fast-food Restaurants*. https://storymaps.arcgis.com/stories/c85cf89d1516498d9645c325b3f5d814"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n",
    "\n",
    "We hypothesize that McDonald’s locations will be more densely concentrated in or near food desert census tracts compared to Chipotle locations. Specifically, we predict a positive association between food desert status and proximity to McDonald’s, and a weaker or negative association between food desert status and proximity to Chipotle.\n",
    "\n",
    "This prediction is based on prior research showing that lower-income and minority communities (areas more likely to be classified as food deserts) tend to have a higher density of traditional fast-food restaurants. In contrast, fast-casual restaurants such as Chipotle are more commonly located in higher-income areas. Therefore, we expect McDonald’s to be more accessible in food desert regions than Chipotle.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with descriptions of your actual datasets.\n",
    "\n",
    "For each dataset include the following information\n",
    "- Dataset #1\n",
    "  - Dataset Name:\n",
    "  - Link to the dataset:\n",
    "  - Number of observations:\n",
    "  - Number of variables:\n",
    "  - Description of the variables most relevant to this project\n",
    "  - Descriptions of any shortcomings this dataset has with repsect to the project\n",
    "- Dataset #2 (if you have more than one!)\n",
    "  - same as above\n",
    "- etc\n",
    "\n",
    "Each dataset deserves either a set of bullet points as above or a few sentences if you prefer that method.\n",
    "\n",
    "If you plan to use multiple datasets, add a few sentences about how you plan to combine these datasets.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. The ideal dataset I would want to answer this question would have the variables zip code, food desert status (is a food desert or is not a food desert), poverty rate, has_McDonald's (whether the area has a McDonald's), and has_Chipotle (whether the area has a Chipotle). We would need to have 7000 observations. These data would be collected using datasets of the McDonald's and Chipotle locations and the US census data covering whether certain areas are food deserts or not. These data would be stored in a .csv or .xlsx file, so I would be organized in a semi-structured way.\n",
    "   \n",
    "2. The datasets are below:\n",
    "   1. Dataset 1 (link https://www.ers.usda.gov/data-products/food-access-research-atlas) - A table that contains the official US census data as of 2010 for food deserts. This dataset uses census tract for the location scaling, and has the poverty rate and population of each census tract. This dataset is publicly available.\n",
    "   2. Dataset 2 (link https://www.kaggle.com/datasets/jacopomazzoni/mcdonalds-locations-2025/data) - A table containing all of the McDonald's locations as of 2025. We will use the zip code column to match McDonald's locations to food desert locations. This dataset is publicly available.\n",
    "   3. Dataset 3 (link https://www.kaggle.com/datasets/jeffreybraun/chipotle-locations) - A table containing all of the Chipotle locations as of 2020. We will use the address column to match Chipotle locations to food desert locations. This dataset is publicly available.\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (2.32.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.67.3)\n",
      "Requirement already satisfied: gdown in /usr/local/python/3.12.1/lib/python3.12/site-packages (5.2.1)\n",
      "Requirement already satisfied: pandas in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/codespace/.local/lib/python3.12/site-packages (from gdown) (4.14.2)\n",
      "Requirement already satisfied: filelock in /usr/local/python/3.12.1/lib/python3.12/site-packages (from gdown) (3.24.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pandas) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.12/site-packages (from beautifulsoup4->gdown) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from beautifulsoup4->gdown) (4.15.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Download Progress:   0%|          | 0/4 [00:00<?, ?it/s]Downloading...\n",
      "From: https://drive.google.com/uc?id=1oMl4m9py5hXGuqA-PodOLejLrxdv9CHx\n",
      "To: /workspaces/Group085_WI26/data/00-raw/us_census_food_desert_data_2010.csv\n",
      "100%|██████████| 47.9M/47.9M [00:01<00:00, 46.2MB/s]\n",
      "Overall Download Progress:  25%|██▌       | 1/4 [00:02<00:08,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: us_census_food_desert_data_2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1pAxxvfxl8hRdFJ-3GGnf0lmAeA18t9nJ\n",
      "To: /workspaces/Group085_WI26/data/00-raw/us_census_food_desert_var_lookup.csv\n",
      "100%|██████████| 21.5k/21.5k [00:00<00:00, 6.01MB/s]\n",
      "Overall Download Progress:  50%|█████     | 2/4 [00:04<00:04,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: us_census_food_desert_var_lookup.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Z8FcNKAD1GJz91MBwXPYr6GN0Mh9obZ5\n",
      "To: /workspaces/Group085_WI26/data/00-raw/mcdonalds_locations_2025.csv\n",
      "100%|██████████| 263k/263k [00:00<00:00, 1.19MB/s]\n",
      "Overall Download Progress:  75%|███████▌  | 3/4 [00:07<00:02,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: mcdonalds_locations_2025.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1mj2jzE54MBWVcSxQmlgZdPAerJBPfeY1\n",
      "To: /workspaces/Group085_WI26/data/00-raw/chipotle_locations_2020.csv\n",
      "100%|██████████| 1.28M/1.28M [00:00<00:00, 3.85MB/s]\n",
      "Overall Download Progress: 100%|██████████| 4/4 [00:09<00:00,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: chipotle_locations_2020.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "%pip install requests tqdm gdown pandas re\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "import find_tract # this is where we find the 2010 census tract for an address\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "datafiles = [\n",
    "    { 'url': 'https://drive.google.com/uc?id=1oMl4m9py5hXGuqA-PodOLejLrxdv9CHx', 'filename':'us_census_food_desert_data_2010.csv'},\n",
    "    { 'url': 'https://drive.google.com/uc?id=1pAxxvfxl8hRdFJ-3GGnf0lmAeA18t9nJ', 'filename':'us_census_food_desert_var_lookup.csv'},\n",
    "    { 'url': 'https://drive.google.com/uc?id=1Z8FcNKAD1GJz91MBwXPYr6GN0Mh9obZ5', 'filename':'mcdonalds_locations_2025.csv'},\n",
    "    { 'url': 'https://drive.google.com/uc?id=1mj2jzE54MBWVcSxQmlgZdPAerJBPfeY1', 'filename':'chipotle_locations_2020.csv'}\n",
    "]\n",
    "\n",
    "get_data.get_gdrive(datafiles,destination_directory='data/00-raw/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1 \n",
    "\n",
    "Instructions: \n",
    "1. Change the header from Dataset #1 to something more descriptive of the dataset\n",
    "2. Write a few paragraphs about this dataset. Make sure to cover\n",
    "   1. Describe the important metrics, what units they are in, and giv some sense of what they mean.  For example \"Fasting blood glucose in units of mg glucose per deciliter of blood.  Normal values for healthy individuals range from 70 to 100 mg/dL.  Values 100-125 are prediabetic and values >125mg/dL indicate diabetes. Values <70 indicate hypoglycemia. Fasting idicates the patient hasn't eaten in the last 8 hours.  If blood glucose is >250 or <50 at any time (regardless of the time of last meal) the patient's life may be in immediate danger\"\n",
    "   2. If there are any major concerns with the dataset, describe them. For example \"Dataset is composed of people who are serious enough about eating healthy that they voluntarily downloaded an app dedicated to tracking their eating patterns. This sample is likely biased because of that self-selection. These people own smartphones and may be healthier and may have more disposable income than the average person.  Those who voluntarily log conscientiously and for long amounts of time are also likely even more interested in health than those who download the app and only log a bit before getting tired of it\"\n",
    "3. Use the cell below to \n",
    "    1. load the dataset \n",
    "    2. make the dataset tidy or demonstrate that it was already tidy\n",
    "    3. demonstrate the size of the dataset\n",
    "    4. find out how much data is missing, where its missing, and if its missing at random or seems to have any systematic relationships in its missingness\n",
    "    5. find and flag any outliers or suspicious entries\n",
    "    6. clean the data or demonstrate that it was already clean.  You may choose how to deal with missingness (dropna of fillna... how='any' or 'all') and you should justify your choice in some way\n",
    "    7. You will load raw data from `data/00-raw/`, you will (optionally) write intermediate stages of your work to `data/01-interim` and you will write the final fully wrangled version of your data to `data/02-processed`\n",
    "4. Optionally you can also show some summary statistics for variables that you think are important to the project\n",
    "5. Feel free to add more cells here if that's helpful for you\n",
    "\n",
    "\n",
    "###McDonalds Locations Dataset\n",
    "\n",
    "Paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               346 w magnolia ave, auburn, al 36832\n",
      "1                300 20th st s, birmingham, al 35233\n",
      "2               3220 morrow rd, birmingham, al 35235\n",
      "3             4719 highway 280, birmingham, al 35242\n",
      "4            1821 cherokee ave sw, cullman, al 35055\n",
      "                            ...                     \n",
      "2624    9370 76th st # b, pleasant prairie, wi 53158\n",
      "2625    2711 n mayfair rd ste a, wauwatosa, wi 53222\n",
      "2626                1204 19th ave n, fargo, nd 58102\n",
      "2627                 1680 45th st s, fargo, nd 58103\n",
      "2628        1508 dell range blvd, cheyenne, wy 82009\n",
      "Name: clean_address, Length: 2629, dtype: str\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n",
    "%reload_ext autoreload\n",
    "mcdonalds_dataset = pd.read_csv(\"data/00-raw/mcdonalds_locations_2025.csv\")\n",
    "\n",
    "# fixing incorrect location names\n",
    "mcdonalds_dataset[\"location\"] = mcdonalds_dataset[\"location\"].replace(\"Washington DC\", \"Washington, DC\")\n",
    "\n",
    "# creating clean addresses for the census geocoder api\n",
    "mcdonalds_dataset[\"clean_address\"] = mcdonalds_dataset.apply(\n",
    "    lambda row: find_tract.clean_address_mcdonalds(row[\"address\"], row[\"location\"]), axis=1\n",
    ")\n",
    "\n",
    "# modifying columns to structure them for batch geocoding\n",
    "mcdonalds_dataset.rename(columns={'location': 'city'}, inplace=True)\n",
    "mcdonalds_dataset[\"street\"] = mcdonalds_dataset[\"clean_address\"].apply(lambda x: x.split(\",\")[0])\n",
    "mcdonalds_dataset[\"zip\"] = mcdonalds_dataset[\"clean_address\"].apply(lambda x: x.split(\",\")[-1].split()[-1])\n",
    "mcdonalds_dataset[\"id\"] = range(1, len(mcdonalds_dataset) + 1)\n",
    "\n",
    "num_datasets = 1\n",
    "while (10000*num_datasets < mcdonalds_dataset.size):\n",
    "    num_datasets += 1\n",
    "# creating an intermediate file for batch geocoding with the census geocoder api\n",
    "for i in range(num_datasets):\n",
    "    mcdonalds_dataset[[\"id\", \"street\", \"city\", \"state\", \"zip\"]].iloc[10000*i:10000*(i+1)].to_csv(f\"data/01-interim/mcdonalds_batch_{i}.csv\", index=False)\n",
    "    find_tract.find_tract_batch(f\"data/01-interim/mcdonalds_batch_{i}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             address         location\n",
      "2072  620 W Dekalb Pike King of Prussia, PA 19406 US  King Of Prussia\n"
     ]
    }
   ],
   "source": [
    "# finding rows that have mismatches between locations and addresses\n",
    "bad_rows = mcdonalds_dataset[\n",
    "    ~mcdonalds_dataset.apply(\n",
    "        lambda row: isinstance(row[\"address\"], str)\n",
    "        and isinstance(row[\"location\"], str)\n",
    "        and (\" \" + row[\"location\"] in row[\"address\"]),\n",
    "        axis=1\n",
    "    )\n",
    "]\n",
    "print(bad_rows[[\"address\", \"location\"]].head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #2 \n",
    "\n",
    "See instructions above for Dataset #1.  Feel free to keep adding as many more datasets as you need.  Put each new dataset in its own section just like these. \n",
    "\n",
    "Lastly if you do have multiple datasets, add another section where you demonstrate how you will join, align, cross-reference or whatever to combine data from the different datasets\n",
    "\n",
    "Please note that you can always keep adding more datasets in the future if these datasets you turn in for the checkpoint aren't sufficient.  The goal here is demonstrate that you can obtain and wrangle data.  You are not tied down to only use what you turn in right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #3 \n",
    "\n",
    "See instructions above for Dataset #1.  Feel free to keep adding as many more datasets as you need.  Put each new dataset in its own section just like these. \n",
    "\n",
    "Lastly if you do have multiple datasets, add another section where you demonstrate how you will join, align, cross-reference or whatever to combine data from the different datasets\n",
    "\n",
    "Please note that you can always keep adding more datasets in the future if these datasets you turn in for the checkpoint aren't sufficient.  The goal here is demonstrate that you can obtain and wrangle data.  You are not tied down to only use what you turn in right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n",
    "\n",
    "## Ethics \n",
    "\n",
    "Instructions: Keep the contents of this cell. For each item on the checklist\n",
    "-  put an X there if you've considered the item\n",
    "-  IF THE ITEM IS RELEVANT place a short paragraph after the checklist item discussing the issue.\n",
    "  \n",
    "Items on this checklist are meant to provoke discussion among good-faith actors who take their ethical responsibilities seriously. Your teams will document these discussions and decisions for posterity using this section.  You don't have to solve these problems, you just have to acknowledge any potential harm no matter how unlikely.\n",
    "\n",
    "Here is a [list of real world examples](https://deon.drivendata.org/examples/) for each item in the checklist that can refer to.\n",
    "\n",
    "[![Deon badge](https://img.shields.io/badge/ethics%20checklist-deon-brightgreen.svg?style=popout-square)](http://deon.drivendata.org/)\n",
    "\n",
    "### A. Data Collection\n",
    " - [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
    "\n",
    "Population data was obtained from the 2010 Census of the Population, where participation is a legal requirement rather than an opt-in process. Additionally, the datasets used are publicly released and intended for research and policy analysis, aligning with established ethical standards for secondary data use.\n",
    "\n",
    " - [X] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
    "\n",
    "According to the 2010 Census Coverage Measurement (CCM) Survey, two major sources of bias were identified. First, undercounting occurred among newborns, toddlers, and minority populations (primarily Black and Hispanic communities), often due to assumptions about household composition and residence in hard-to-count housing areas. Second, overcounting occurred in wealthier areas where individuals owned multiple properties, as well as among college students who were counted both at their family residence and their school address. While national population totals remain close to accurate, these issues can distort geographic and demographic representation at local levels, resulting in distributional bias.\n",
    "\n",
    " - [X] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    "\n",
    "The population data used, including age, race, Hispanic ethnicity, and residence in group quarters, are derived from the 2010 Census of the Population and are aggregated into 0.5-kilometer-square grid cells. This spatial aggregation prevents identification of exact locations and individuals, thereby minimizing the risk of exposing personally identifiable information.\n",
    "\n",
    " - [X] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
    "\n",
    "This project recognizes that food deserts often overlap with low-income communities and communities of color. As a result, the analysis focuses on income, zoning, and business placement decisions, and findings are presented as geographic patterns rather than claims about individual behavior or health choices.\n",
    "\n",
    "### B. Data Storage\n",
    " - [X] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    "\n",
    "The data used in this analysis is publicly available and aggregated at the census tract level, meaning it does not contain any personally identifiable information. As a result, risks related to individual data exposure are minimal. That said, it is still important to store datasets securely and limit access to project members to prevent misuse or misinterpretation of sensitive neighborhood-level indicators. Even aggregated data about food deserts and socioeconomic conditions can carry social implications if taken out of context, so maintaining basic data security and responsible handling practices helps ensure the analysis is used appropriately.\n",
    "\n",
    " - [X] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    "\n",
    "Because this project relies exclusively on de-identified, aggregate-level public data, there is no mechanism through which individuals can be identified or request removal of personal information. However, this also means that individual communities may still be indirectly represented in ways that carry social meaning. Being mindful of how findings are framed is important to avoid reinforcing stereotypes or deficit-based narratives about low-income or minority neighborhoods.\n",
    "\n",
    " - [X] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    "\n",
    "The datasets used for this project will only be retained for the duration of the analysis and reporting process and do not require long-term storage beyond the scope of the class project. Since the data is publicly available and non-sensitive, retention does not pose a privacy risk, but limiting prolonged storage helps reinforce responsible data practices and reduces the chance of future misuse or misinterpretation outside of the original research context.\n",
    "\n",
    "### C. Analysis\n",
    " - [ ] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n",
    " - [ ] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n",
    " - [ ] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
    " - [ ] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
    " - [ ] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
    "\n",
    "### D. Modeling\n",
    " - [ ] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
    " - [ ] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
    " - [ ] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
    " - [ ] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
    " - [ ] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
    "\n",
    "### E. Deployment\n",
    " - [ ] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
    " - [ ] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
    " - [ ] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
    " - [ ] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n",
    "\n",
    "\n",
    "* *Meet every Wednesday at 7pm to discuss about the project*\n",
    "* *Complete the assigned individual tasks before each meet up*\n",
    "* *During meet up, discuss the viability of each section completed, propose changes if needed*\n",
    "* *Assign tasks after each meet up*\n",
    "* *Keep group members up to date. Inform any difficulties in the groupchat*\n",
    "* *Help members if possible*\n",
    "* *Keep everyone noticed if a time does not work or anything unaccomplishable*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: Replace this with your timeline.  **PLEASE UPDATE your Timeline!** No battle plan survives contact with the enemy, so make sure we understand how your plans have changed.  Also if you have lost points on the previous checkpoint fix them\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/11  |  7 PM | Clean the data, merge any necessary dataframes | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part | \n",
    "| 2/18 |  7 PM |  Finalize datasets, finish up any incompleted data cleaning & wrangling | Review/Edit wrangling/EDA; Discuss Analysis Plan | \n",
    "| 2/25  | 7 PM  | Import & Wrangle Data (Ant Man); EDA (Hulk)  | Discuss/edit Analysis; Complete project check-in  |\n",
    "| 3/4  | 7 PM  | Finish up EDA; Begin Analysis| Review/Edit wrangling/EDA; Discuss Analysis Plan  |\n",
    "| 3/11  | 7 PM  | Finish Analysis: Draft results/conclusion/discussion | Discuss/edit Analysis; Complete project check-in |\n",
    "| 3/18  | 7 PM  | Wrap Up, finish any incompleted work | Turn in Final Project & Group Project Surveys!!! |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
